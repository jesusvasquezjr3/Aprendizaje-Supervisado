{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f7664f",
   "metadata": {},
   "source": [
    "## 2.6 Pipelines\n",
    "\n",
    "### Problema\n",
    "\n",
    "Tenemos nuestro modelo en producción. Nuestro API de Flask carga nuestro modelo, y tenemos un endpoint para enviar datos y realizar alguna predicción con éstos:\n",
    "\n",
    "Lamentablemetne, nuestro API no es el más práctico del mundo. Para empezar, el formato en el que pide los datos es muy poco amigable. El JSON de entrada podría ser algo así:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:5000/predecir -d '{\"input\": [0,1,0.6159084,0,0,0.55547282,1]}'\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Pclass\": 0,\n",
    "  \"Sex\": 1,\n",
    "  \"Age\": 0.6159084,\n",
    "  \"SibSp\": 0,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 0.55547282,\n",
    "  \"Embarked\": 1\n",
    "}\n",
    "```\n",
    "\n",
    "Esto estaría mucho mejor porque mínimo sabríamos los valores que estamos asignando a cada feature. No obstante,\n",
    "\n",
    "Recordemos que en nuestro proceso de limpieza y feature engineering, realizamos transformaciones significativas a los datos: conversión a variables numéricas, normalización y escalamiento.\n",
    "\n",
    "Por supuesto que no podemos esperar que los usuarios de nuestro API realicen estas transformaciones por su propia cuenta.\n",
    "\n",
    "El objetivo final de esta implementación será que podamos usar nuestro API con un JSON de entrada como el siguiente:\n",
    "\n",
    "Es decir, sin transformaciones.\n",
    "\n",
    "Podríamos creer que implementaremos lógica en nuestro API para realizar transformaciones antes de hacer inferencia, pero la implementación que seguiremos es mucho más simple y elegante que eso.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Pclass\": 2,\n",
    "  \"Sex\": \"male\",\n",
    "  \"Age\": 46,\n",
    "  \"SibSp\": 0,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 7.2500,\n",
    "  \"Embarked\": \"C\"\n",
    "}\n",
    "```\n",
    "\n",
    "¿Qué significa una edad de 0.6159084?\n",
    "\n",
    "Lo que haremos a continuación es utilizar el objeto **Pipeline** de *Scikit Learn*, el cual, como su nombre lo sugiere, creará las directrices que permitirán el flujo de datos desde su entrada hasta su salida en forma de predicciones. La siguiente imagen lo ilustra muy bien:\n",
    "\n",
    "*Fuente: Ravi Teja Kandimalla*\n",
    "\n",
    "El código que escribiremos es parecido al que hemos escrito anteriormente, pero reescribiremos ciertas partes para que funcione correctamente el pipeline.\n",
    "\n",
    "Crearemos nuestro Pipeline en un nuevo notebook. Abre el notebook `6_pipeline.ipynb`. Usaremos los datos que dejamos en el directorio `clean/`.\n",
    "\n",
    "---\n",
    "\n",
    "### Importar paquetes\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder # <------------------ Esto es nuevo :)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer # <------------------ Esto es nuevo :)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline # <------------------ Esto es nuevo :)\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "# Métricas de evaluación\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Para guardar el modelo\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('./data/titanic_clean.csv')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocesamiento\n",
    "\n",
    "Vemos aquí algo nuevo y algo “viejo”. En nuestros notebooks anteriores, usamos `QuantileTransformer` para normalizar las columnas Age y Fare. Posteriormente, creamos un nuevo DataFrame y lo guardamos con estas nuevas transformaciones.\n",
    "\n",
    "El objeto **ColumnTransformer** define cuáles y cómo serán las transformaciones que se harán a los datos. Digamos que es nuestro proceso de *feature engineering* en un solo objeto.\n",
    "\n",
    "En notebooks anteriores usamos `LabelEncoder`. No profundicemos en las diferencias técnicas. Para propósitos de nuestro proyecto `OneHotEncoder` y `LabelEncoder` hacen lo mismo. Tenemos que usar `OneHotEncoder` porque `LabelEncoder` no funciona con Pipelines.\n",
    "\n",
    "```python\n",
    "# Definir preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(), ['Sex', 'Embarked']),\n",
    "        ('age', QuantileTransformer(output_distribution='normal', n_quantiles=500), ['Age']),\n",
    "        ('fare', QuantileTransformer(output_distribution='normal', n_quantiles=500), ['Fare'])\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantener otras columnas sin cambios\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Definición de modelos\n",
    "\n",
    "Nuestro diccionario de modelos tiene un cambio pequeño:\n",
    "\n",
    "```python\n",
    "modelos = {\n",
    "    'Regresión Logística': {\n",
    "        'modelo': LogisticRegression(),\n",
    "        'parametros': {\n",
    "            'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'model__penalty': ['l1', 'l2'],\n",
    "            'model__solver': ['liblinear', 'saga'],\n",
    "            'model__max_iter': [100, 500, 1000]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Vectores de Soporte': {\n",
    "        'modelo': SVC(),\n",
    "        'parametros': {\n",
    "            'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'model__C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Árbol de Decisión': {\n",
    "        'modelo': DecisionTreeClassifier(),\n",
    "        'parametros': {\n",
    "            'model__splitter': ['best', 'random'],\n",
    "            'model__max_depth': [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Bosques Aleatorios': {\n",
    "        'modelo': RandomForestClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [10, 100],\n",
    "            'model__max_depth': [None, 1, 2, 3, 4],\n",
    "            'model__max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Gradient Boosting': {\n",
    "        'modelo': GradientBoostingClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [10, 100],\n",
    "            'model__max_depth': [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador AdaBoost': {\n",
    "        'modelo': AdaBoostClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [10, 100]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador K-Nearest Neighbors': {\n",
    "        'modelo': KNeighborsClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_neighbors': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador XGBoost': {\n",
    "        'modelo': XGBClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [10, 100],\n",
    "            'model__max_depth': [None, 1, 2, 3]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador LGBM': {\n",
    "        'modelo': LGBMClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [10, 100],\n",
    "            'model__max_depth': [None, 1, 2, 3],\n",
    "            'model__learning_rate': [0.1, 0.2, 0.3],\n",
    "            'model__verbose': [-1]\n",
    "        }\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'modelo': GaussianNB(),\n",
    "        'parametros': {}\n",
    "    },\n",
    "    'Clasificador Naive Bayes': {\n",
    "        'modelo': BernoulliNB(),\n",
    "        'parametros': {\n",
    "            'model__alpha': [0.1, 1.0, 10.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### División de datos\n",
    "\n",
    "Ningún cambio significativo en esta etapa.\n",
    "\n",
    "```python\n",
    "X = df.drop(['Survived'], axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "```\n",
    "\n",
    "### Variables auxiliares\n",
    "\n",
    "Ningún cambio significativo en esta etapa.\n",
    "\n",
    "---\n",
    "\n",
    "### Ciclo for de GridSearch\n",
    "\n",
    "El contenido de nuestro ciclo for sí cambiará un poco:\n",
    "\n",
    "```python\n",
    "puntajes_modelos = []\n",
    "mejor_precision = 0\n",
    "mejor_estimador = None\n",
    "mejor_modelo = None\n",
    "estimadores = {}\n",
    "\n",
    "for nombre, info_modelo in modelos.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', MinMaxScaler()),  # MinMaxScaler se aplica a todas las columnas después del pre\n",
    "        ('model', info_modelo['modelo'])\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=info_modelo['parametros'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    puntajes_modelos.append({\n",
    "        'Modelo': nombre,\n",
    "        'Precisión': precision\n",
    "    })\n",
    "\n",
    "    estimadores[nombre] = grid_search.best_estimator_\n",
    "\n",
    "    if precision > mejor_precision:\n",
    "        mejor_modelo = nombre\n",
    "        mejor_precision = precision\n",
    "        mejor_estimador = grid_search.best_estimator_\n",
    "```\n",
    "\n",
    "Los pasos (`steps`) que especificamos en este pipeline son:\n",
    "\n",
    "1. Preprocesar los datos haciendo uso del objeto `preprocessor` que creamos anteriormente.\n",
    "2. Escalamiento de datos usando `MinMaxScaler`.\n",
    "3. Modelo.\n",
    "\n",
    "Posteriormente, usaremos este nuevo objeto llamado `pipeline` como si fuera el modelo o estimador.\n",
    "\n",
    "Vemos que esto es prácticamente igual a lo que hacíamos antes. La diferencia está en que `estimator` ya no es un modelo individual sino un objeto de tipo `Pipeline`.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostrar resultados y guardar modelo\n",
    "\n",
    "Lo último no sufre ningún cambio:\n",
    "\n",
    "```python\n",
    "metricas = pd.DataFrame(puntajes_modelos).sort_values('Precisión', ascending=False)\n",
    "\n",
    "print(\"Rendimiento de los modelos de clasificación\")\n",
    "print(metricas.round(2))\n",
    "\n",
    "print('---------------------------------------------------')\n",
    "print(\"MEJOR MODELO DE CLASIFICACIÓN\")\n",
    "print(f\"Modelo: {mejor_modelo}\")\n",
    "print(f\"Precisión: {mejor_precision:.2f}\")\n",
    "\n",
    "with open('pipeline.pkl', 'wb') as archivo_estimador:\n",
    "    pickle.dump(mejor_estimador, archivo_estimador)\n",
    "```\n",
    "\n",
    "#### Warnings\n",
    "\n",
    "Es muy probable que nuestro GridSearch arroje varios *warnings*:\n",
    "\n",
    "> The max\\_iter was reached which means the coef\\_ did not converge\n",
    "> The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME alg\n",
    "\n",
    "Es importante leer los warnings, pero no hay que preocuparnos tanto por ellos. Indican que en alguno de los hiperparámetros probados no fueron adecuados.\n",
    "\n",
    "Aunque el código es el mismo, cuando guardamos nuestro pickle, estamos guardando un **pipeline**, no un modelo individual.\n",
    "\n",
    "---\n",
    "\n",
    "### Refactorizar API\n",
    "\n",
    "Finalmente, modificaremos el código de nuestro API para que funcione con este pipeline. Recordemos que el objetivo será usar JSONs de entrada como éste:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Pclass\": 2,\n",
    "  \"Sex\": \"male\",\n",
    "  \"Age\": 46,\n",
    "  \"SibSp\": 0,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 7.2500,\n",
    "  \"Embarked\": \"C\"\n",
    "}\n",
    "```\n",
    "\n",
    "Nuestro archivo `app.py` ahora es el siguiente:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Cargar el modelo guardado\n",
    "with open('pipeline.pkl', 'rb') as archivo_modelo:\n",
    "    modelo = pickle.load(archivo_modelo)\n",
    "\n",
    "@app.route('/predecir', methods=['POST'])\n",
    "def predecir():\n",
    "    # Obtener los datos de la solicitud\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Crear un DataFrame de pandas a partir del JSON\n",
    "    input_data = pd.DataFrame([data])\n",
    "\n",
    "    # Hacer la predicción usando el modelo que tiene el pipeline que hará la transformación\n",
    "    prediccion = modelo.predict(input_data)\n",
    "\n",
    "    # Devolver la predicción como JSON\n",
    "    output = {'Survived': int(prediccion[0])}\n",
    "    return jsonify(output)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "Si probamos nuestra app de Flask ahora con el JSON de entrada deseado, veremos que funciona perfectamente:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://127.0.0.1:5000/predecir -H 'Content-Type: application/json' -d '{\n",
    "  \"Pclass\": 2,\n",
    "  \"Sex\": \"male\",\n",
    "  \"Age\": 46,\n",
    "  \"SibSp\": 0,\n",
    "  \"Parch\": 0,\n",
    "  \"Fare\": 7.2500,\n",
    "  \"Embarked\": \"C\"\n",
    "}'\n",
    "```\n",
    "\n",
    "¡Nuestro Pipeline funciona perfectamente!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenando: Regresión Logística ===\n",
      "Mejores params: {'model__C': 0.01, 'model__max_iter': 100, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
      "Accuracy (test): 0.8324\n",
      "\n",
      "=== Entrenando: Clasificador de Vectores de Soporte ===\n",
      "Mejores params: {'model__C': 1, 'model__kernel': 'poly'}\n",
      "Accuracy (test): 0.8380\n",
      "\n",
      "=== Entrenando: Clasificador de Árbol de Decisión ===\n",
      "Mejores params: {'model__max_depth': 4, 'model__splitter': 'random'}\n",
      "Accuracy (test): 0.8436\n",
      "\n",
      "=== Entrenando: Clasificador de Bosques Aleatorios ===\n",
      "Mejores params: {'model__max_depth': 7, 'model__max_features': 'sqrt', 'model__n_estimators': 50}\n",
      "Accuracy (test): 0.8827\n",
      "\n",
      "=== Entrenando: Clasificador de Gradient Boosting ===\n",
      "Mejores params: {'model__max_depth': 3, 'model__n_estimators': 100}\n",
      "Accuracy (test): 0.8492\n",
      "\n",
      "=== Entrenando: Clasificador AdaBoost ===\n",
      "Mejores params: {'model__n_estimators': 200}\n",
      "Accuracy (test): 0.8212\n",
      "\n",
      "=== Entrenando: Clasificador K-Nearest Neighbors ===\n",
      "Mejores params: {'model__n_neighbors': 9}\n",
      "Accuracy (test): 0.8045\n",
      "\n",
      "=== Entrenando: GaussianNB ===\n",
      "Mejores params: {}\n",
      "Accuracy (test): 0.7933\n",
      "\n",
      "=== Entrenando: Clasificador Naive Bayes (Bernoulli) ===\n",
      "Mejores params: {'model__alpha': 0.1}\n",
      "Accuracy (test): 0.8101\n",
      "\n",
      "=== Entrenando: Clasificador XGBoost ===\n",
      "Mejores params: {'model__max_depth': 2, 'model__n_estimators': 200}\n",
      "Accuracy (test): 0.8212\n",
      "\n",
      "=== Entrenando: Clasificador LGBM ===\n",
      "Mejores params: {'model__learning_rate': 0.05, 'model__max_depth': 2, 'model__n_estimators': 200, 'model__verbose': -1}\n",
      "Accuracy (test): 0.8212\n",
      "\n",
      "Rendimiento de los modelos de clasificación\n",
      "                                  Modelo  Precisión\n",
      "3     Clasificador de Bosques Aleatorios     0.8827\n",
      "4      Clasificador de Gradient Boosting     0.8492\n",
      "2      Clasificador de Árbol de Decisión     0.8436\n",
      "1    Clasificador de Vectores de Soporte     0.8380\n",
      "0                    Regresión Logística     0.8324\n",
      "5                  Clasificador AdaBoost     0.8212\n",
      "10                     Clasificador LGBM     0.8212\n",
      "9                   Clasificador XGBoost     0.8212\n",
      "8   Clasificador Naive Bayes (Bernoulli)     0.8101\n",
      "6       Clasificador K-Nearest Neighbors     0.8045\n",
      "7                             GaussianNB     0.7933\n",
      "---------------------------------------------------\n",
      "MEJOR MODELO DE CLASIFICACIÓN\n",
      "Modelo: Clasificador de Bosques Aleatorios\n",
      "Precisión: 0.8827\n",
      "\n",
      "✅ Guardado: pipeline.pkl (contiene TODO el Pipeline: preprocesamiento + modelo).\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2.6 Pipelines - Entrenamiento\n",
    "# =========================\n",
    "\n",
    "# ---- Importar paquetes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos base de sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "\n",
    "# Métrica\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Guardado\n",
    "import pickle\n",
    "\n",
    "# ---- (Opcional) Modelos adicionales: XGBoost y LightGBM\n",
    "have_xgb = False\n",
    "have_lgbm = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    have_xgb = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    have_lgbm = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Carga de datos\n",
    "# -----------------------\n",
    "# Asegúrate de que la ruta exista y el CSV contenga las columnas esperadas:\n",
    "# ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Survived']\n",
    "df = pd.read_csv('./data/titanic_clean.csv')\n",
    "\n",
    "# -----------------------\n",
    "# Preprocesamiento\n",
    "# -----------------------\n",
    "# - OneHotEncoder para columnas categóricas\n",
    "# - QuantileTransformer para 'Age' y 'Fare'\n",
    "# - remainder='passthrough' mantiene el resto de columnas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'), ['Sex', 'Embarked']),\n",
    "        ('age', QuantileTransformer(output_distribution='normal', n_quantiles=500, subsample=int(1e9)), ['Age']),\n",
    "        ('fare', QuantileTransformer(output_distribution='normal', n_quantiles=500, subsample=int(1e9)), ['Fare']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Definición de modelos e hiperparámetros\n",
    "# Los hiperparámetros se referencian con el prefijo 'model__'\n",
    "# porque el estimador final está dentro del Pipeline en el paso 'model'.\n",
    "# -----------------------\n",
    "modelos = {\n",
    "    'Regresión Logística': {\n",
    "        'modelo': LogisticRegression(),\n",
    "        'parametros': {\n",
    "            'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'model__penalty': ['l1', 'l2'],\n",
    "            'model__solver': ['liblinear', 'saga'],\n",
    "            'model__max_iter': [100, 500, 1000]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Vectores de Soporte': {\n",
    "        'modelo': SVC(),\n",
    "        'parametros': {\n",
    "            'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'model__C': [0.1, 1, 10]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Árbol de Decisión': {\n",
    "        'modelo': DecisionTreeClassifier(),\n",
    "        'parametros': {\n",
    "            'model__splitter': ['best', 'random'],\n",
    "            'model__max_depth': [None, 1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Bosques Aleatorios': {\n",
    "        'modelo': RandomForestClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [None, 3, 5, 7],\n",
    "            'model__max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador de Gradient Boosting': {\n",
    "        'modelo': GradientBoostingClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [50, 100, 200],\n",
    "            'model__max_depth': [1, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador AdaBoost': {\n",
    "        'modelo': AdaBoostClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [50, 100, 200]\n",
    "        }\n",
    "    },\n",
    "    'Clasificador K-Nearest Neighbors': {\n",
    "        'modelo': KNeighborsClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_neighbors': [3, 5, 7, 9]\n",
    "        }\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'modelo': GaussianNB(),\n",
    "        'parametros': {}  # sin hiperparámetros en este ejemplo\n",
    "    },\n",
    "    'Clasificador Naive Bayes (Bernoulli)': {\n",
    "        'modelo': BernoulliNB(),\n",
    "        'parametros': {\n",
    "            'model__alpha': [0.1, 0.5, 1.0, 10.0]\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Incluir XGBoost si está disponible\n",
    "if have_xgb:\n",
    "    modelos['Clasificador XGBoost'] = {\n",
    "        'modelo': XGBClassifier(\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False,\n",
    "            n_estimators=100,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__max_depth': [2, 3, 4, 5]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Incluir LightGBM si está disponible\n",
    "if have_lgbm:\n",
    "    modelos['Clasificador LGBM'] = {\n",
    "        'modelo': LGBMClassifier(),\n",
    "        'parametros': {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            # En LightGBM, max_depth=-1 es \"sin límite\"\n",
    "            'model__max_depth': [-1, 2, 3, 4],\n",
    "            'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "            'model__verbose': [-1]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# División de datos\n",
    "# -----------------------\n",
    "X = df.drop(['Survived'], axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=100, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Entrenamiento con GridSearch en un bucle por modelo\n",
    "# Cada iteración crea un Pipeline: preprocessor -> MinMaxScaler -> model\n",
    "# Guardamos el mejor Pipeline encontrado (mejor_estimador).\n",
    "# -----------------------\n",
    "puntajes_modelos = []\n",
    "mejor_precision = 0.0\n",
    "mejor_estimador = None\n",
    "mejor_modelo = None\n",
    "estimadores = {}\n",
    "\n",
    "for nombre, info in modelos.items():\n",
    "    print(f\"\\n=== Entrenando: {nombre} ===\")\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('model', info['modelo'])\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=info['parametros'],\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    precision = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Mejores params: {grid_search.best_params_}\")\n",
    "    print(f\"Accuracy (test): {precision:.4f}\")\n",
    "\n",
    "    puntajes_modelos.append({'Modelo': nombre, 'Precisión': precision})\n",
    "    estimadores[nombre] = grid_search.best_estimator_\n",
    "\n",
    "    if precision > mejor_precision:\n",
    "        mejor_precision = precision\n",
    "        mejor_modelo = nombre\n",
    "        mejor_estimador = grid_search.best_estimator_\n",
    "\n",
    "# -----------------------\n",
    "# Resultados\n",
    "# -----------------------\n",
    "metricas = pd.DataFrame(puntajes_modelos).sort_values('Precisión', ascending=False)\n",
    "print(\"\\nRendimiento de los modelos de clasificación\")\n",
    "print(metricas.round(4))\n",
    "print('---------------------------------------------------')\n",
    "print(\"MEJOR MODELO DE CLASIFICACIÓN\")\n",
    "print(f\"Modelo: {mejor_modelo}\")\n",
    "print(f\"Precisión: {mejor_precision:.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Guardar el mejor Pipeline\n",
    "# -----------------------\n",
    "with open('pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(mejor_estimador, f)\n",
    "\n",
    "print(\"\\n✅ Guardado: pipeline.pkl (contiene TODO el Pipeline: preprocesamiento + modelo).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc2834",
   "metadata": {},
   "source": [
    "**Para la prueba:**\n",
    "\n",
    "```shell\n",
    "Invoke-RestMethod -Uri http://127.0.0.1:5000/predecir `\n",
    "    -Method Post `\n",
    "    -ContentType \"application/json\" `\n",
    "    -Body '{\n",
    "        \"Pclass\": 2,\n",
    "        \"Sex\": \"male\",\n",
    "        \"Age\": 46,\n",
    "        \"SibSp\": 0,\n",
    "        \"Parch\": 0,\n",
    "        \"Fare\": 7.2500,\n",
    "        \"Embarked\": \"C\"\n",
    "    }'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nombre_proyecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
